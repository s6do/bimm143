---
title: "Class08 mini-project"
author: "Samuel Do (PID:A15803613)"
date: "2/10/2022"
output: html_document
---
```{r}
# 1) Exploratory Data Analysis
# Input data

fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)

# Alter dataframe to remove "diagnosis" column
wisc.data <- wisc.df[,-1]
head(wisc.data)

# Store Diagnosis column as vector
# First store as factor and then as vector
library(tidyverse)
diagnosis_list <- wisc.df[,1]
diagnosis_level <- c("B", "M")
diagnosis <- factor(diagnosis_list, level=diagnosis_level)
diagnosis

# Factor with warning
diagnosis2 <- parse_factor(diagnosis_list, level=diagnosis_level)
```
```{r}
# [Q1] How many observations are in this dataset?
nrow(wisc.data)
# There are 569 observations in the dataset.

# [Q2] How many of the observations have a malignant diagnosis?
table(diagnosis)
# There are 212 malignant diagnosis observations.

# [Q3] How many variables/features in the data are suffixed with _mean?
length(grep("_mean", colnames(wisc.df)))
# There are 10 variables in the data suffixed with _mean.
```
```{r}
# 2) Principal Component Analysis

# Check if wisc.data needs to be scaled
colMeans(wisc.data)
apply(wisc.data,2,sd)
# Does need to be scaled due to high variance.

# Perform PCA on wisc.data
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
```{r}
# [Q4] From your results, what proportion of the original variance is captured by the first principal components (PC1)?
# On the summary(wisc.pr), the proportion of variance that PC1 captures is 0.4427. 

# [Q5] How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
# Based on the summary(wisc.pr), 3 principal components are required to describe at least 70% of the original variance in the data.

# [Q6] How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
# Based on the summary(wisc.pr), 7 principal components are required to describe at least 90% of the original variance in the data. 
```
```{r}
#Biplot of PCA results
biplot(wisc.pr)

# [Q7] What stands out to you about this plot? Is it easy or difficult to understand? Why?
# Based on observations of the plot, a majority of PC1 and PC2 have variances that fall in between -0.1 and 0.1. However, a few IDs, such as 8710441, have variances not between -0.1 and 0.1. The biplot is difficult to understand because of how compact all the data is, making it difficult to read. 
```
```{r}
# Scatterplot of PC1 and PC2
plot(wisc.pr$x[,1:2], col=diagnosis,
  xlab="PC1", ylab="PC2")

# [Q8] Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis,
  xlab="PC1", ylab="PC3")
#It appears that PC3  has less variance than the PC2 plot since the more variance points for PC3 fall between -5 and 5 than PC2.
```
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=df$diagnosis) + 
  geom_point()

```
```{r}
#Calculate variance of each components
var.pr <- wisc.pr$sdev^2
head(var.pr)

# Variance explained by each principal component: pve
pve <- var.pr/sum(head(var.pr))

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
```{r}
# [Q9] For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
wisc.pr$rotation[,1]
# The component of the loading vector for concave.poins_mean is -0.26085376.

# [Q10] What is the minimum number of principal components required to explain 80% of the variance of the data?
summary(wisc.pr)
# 5 prinicipal components are required to explain 80% of the variance of the data.
```
```{r}
# 3) Hierarchial Clustering

#Scale wisc.data 
data.scaled <- scale(wisc.data)

# Calculate Euclidean distances between all pairs of observations in scaled dataset
data.dist <- dist(data.scaled)

# Create hierarchical clustering model using complete linkage
wisc.hclust <- hclust(data.dist, method="complete")

# [Q11] Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
plot(wisc.hclust)
  abline(h=19, col="red", lty=2)
```
```{r}
#Selecting number of clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)

#[Q12] Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
# The best cluster vs diagnoses match is found by cutting into 4 clusters
```
```{r}
wisc.hclust.single <- hclust(data.dist, method="single")
wisc.hclust.singles <- cutree(wisc.hclust.single, k=3)
table(wisc.hclust.singles, diagnosis)


wisc.hclust.average <- hclust(data.dist, method="average")
wisc.hclust.averages <- cutree(wisc.hclust.average, k=3)
table(wisc.hclust.averages, diagnosis)


wisc.hclust.wardD2 <- hclust(data.dist, method="ward.D2")
wisc.hclust.ward.D2 <- cutree(wisc.hclust.wardD2, k=9)
table(wisc.hclust.ward.D2, diagnosis)


# [Q13] Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
# The ward.D2 method gave the better results as it was able to separate the data.dist dataset into respective clusters based on diagnosis.
```
```{r}
# K-means clustering
wisc.km <- kmeans(scale(wisc.data), centers=2, nstart=20)
table(wisc.km$cluster, diagnosis)
#Compare to hclust results
table(wisc.hclust.clusters, wisc.km$cluster)
# [Q14] How well does k-means separate the two diagnoses? How does it compare to your hclust results?
# The k-means method separates the two diagnoses fairly well as it designates the benign and malignant diagnoses into separate 

```
```{r}
# 5) Combining methods

# Use ward.D2 method to create hierarchical clustering model
dist <- data.dist
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
plot(wisc.pr.hclust)
```
```{r}
#Determining whether two main clusters indicate malignant and benign diagnoses
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis)

```
```{r}
# Use distance along first 7 PCs for clustering
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
# Comparing to actual diagnosis
table(wisc.pr.hclust.clusters, diagnosis)

# [Q15] How well does the newly created model with four clusters separate out the two diagnoses?
# The new model separates the two diagnosis well as a majority of each diagnosis is separated into one of the clusters

# [Q16] How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses?
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
# Both clustering models created separate the diagnosis into separate clusters very well. 
```
```{r}
# 6) Sensitivity/Specificity 

# Sensitivity: test's ability to correctly detect ill patients with condition; TP/(TP+FN)
# Specificity: test's ability to correctly reject healthy patients w/o condition; TN/(TN+FN)


# [Q17] Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
# The k-means clustering method has the best specificity, while the combined clustering method using ward.D2 and hierarchical clustering had the best sensitivity. 

```
```{r}
# 7) Prediction

# Use predict() to project new cancer cell data onto previous PCA model
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
# Plot new data onto previous PCA model
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")

# [Q18] Which of these new patients should we prioritize for follow up based on your results?
# Patient 2 should be prioritized since patient 1 is likely to be a true negative and therefore have a benign tumor. 
```


